{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TopicModelling.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNWiOft6vgiL"
      },
      "source": [
        "doc1 = \"Writing code seems really cool. Some people like it, some may not.\"\n",
        "doc2 = \"We can understand code better if we constantly practice.\"\n",
        "doc3 = \"We all are from different countries but the same planet.\"\n",
        "doc4 = \"This year, the Olympics are coming, watching the games will be fun.\"\n",
        "doc5 = \"Basketball is a sport.\"\n",
        "\n",
        "# compile documents\n",
        "doc_complete = [doc1, doc2, doc3, doc4, doc5]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdXLBp2twOs8",
        "outputId": "488b8f83-5882-45b1-a24f-a7ab18dc3213"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import string\n",
        "stop = set(stopwords.words('english'))\n",
        "exclude = set(string.punctuation)\n",
        "lemma = WordNetLemmatizer()\n",
        "def clean(doc):\n",
        "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
        "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
        "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
        "    return normalized\n",
        "\n",
        "doc_clean = [clean(doc).split() for doc in doc_complete]       "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9A9yzSvHwRYX"
      },
      "source": [
        "# Importing Gensim\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "\n",
        "dictionary = corpora.Dictionary(doc_clean)\n",
        "\n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn3LUm0GwV6B"
      },
      "source": [
        "# Creating the object for LDA model using gensim library\n",
        "Lda = gensim.models.ldamodel.LdaModel\n",
        "\n",
        "# Running and Trainign LDA model on the document term matrix.\n",
        "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDbVupttwf__",
        "outputId": "e213ae03-588d-4164-96e2-d44128a9e0f7"
      },
      "source": [
        "print(ldamodel.print_topics(num_topics=3, num_words=3))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, '0.117*\"planet\" + 0.117*\"different\" + 0.117*\"country\"'), (1, '0.051*\"not\" + 0.051*\"people\" + 0.051*\"writing\"'), (2, '0.100*\"code\" + 0.100*\"constantly\" + 0.100*\"understand\"')]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}